{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = '../data/porto-seguro-safe-driver-prediction/'\n",
    "\n",
    "train = pd.read_csv(data_path + 'train.csv', index_col = 'id')\n",
    "test = pd.read_csv(data_path + 'test.csv', index_col = 'id')\n",
    "submission = pd.read_csv(data_path + 'sample_submission.csv', index_col= 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train, test], ignore_index=True)\n",
    "all_data = all_data.drop('target', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ps_ind_01', 'ps_ind_02_cat', 'ps_ind_03', 'ps_ind_04_cat',\n",
       "       'ps_ind_05_cat', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin',\n",
       "       'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin',\n",
       "       'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15', 'ps_ind_16_bin',\n",
       "       'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03',\n",
       "       'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_04_cat',\n",
       "       'ps_car_05_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat',\n",
       "       'ps_car_09_cat', 'ps_car_10_cat', 'ps_car_11_cat', 'ps_car_11',\n",
       "       'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15', 'ps_calc_01',\n",
       "       'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06',\n",
       "       'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11',\n",
       "       'ps_calc_12', 'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin',\n",
       "       'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin',\n",
       "       'ps_calc_20_bin'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = all_data.columns\n",
    "all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1488028x184 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 20832392 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_features = [feature for feature in all_features if 'cat' in feature]\n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "\n",
    "encoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features])\n",
    "\n",
    "encoded_cat_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_features = ['ps_ind_14', 'ps_ind_10_bin', 'ps_ind_11_bin',\n",
    "                'ps_ind_12_bin','ps_ind_13_bin','ps_car_14']\n",
    "\n",
    "remaining_features = [feature for feature in all_features\n",
    "                      if ('cat' not in feature and\n",
    "                          'calc' not in feature and\n",
    "                          feature not in drop_features)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "all_data_sprs = sparse.hstack([sparse.csr_matrix(all_data[remaining_features]),\n",
    "                               encoded_cat_matrix], format='csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델링 이전 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(train)\n",
    "\n",
    "X = all_data_sprs[:num_train]\n",
    "X_test = all_data_sprs[num_train:]\n",
    "\n",
    "y = train['target'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eval_gini(y_true, y_pred):\n",
    "    # 실제 값과 예측 값의 크기가 같은지 확인\n",
    "    assert y_true.shape == y_pred.shape\n",
    "\n",
    "    n_samples = y_true.shape[0] # 데이터 개수\n",
    "    L_mid = np.linspace( 1 / n_samples, 1, n_samples) # 대각선\n",
    "\n",
    "    # 1. 예측값에 대한 지니계수\n",
    "    pred_order = y_true[y_pred.argsort()]\n",
    "    L_pred = np.cumsum(pred_order) / np.sum(pred_order)\n",
    "    G_pred = np.sum(L_mid - L_pred)\n",
    "\n",
    "    # 2. 예측이 완벽할 때의 지니계수\n",
    "    true_order = y_true[y_true.argsort()]\n",
    "    L_true = np.cumsum(true_order) / np.sum(pred_order)\n",
    "    G_true = np.sum(L_mid - L_true)\n",
    "\n",
    "    return G_pred / G_true\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    # RETURN: 평가지표 이름, 평가 점수, 평가 지표가 높을수록 좋은지 여부\n",
    "    return 'gini', eval_gini(labels, preds), True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaseLine: OOF 기반으로 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1991)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'objective': 'binary',\n",
    "          'learning_rate': 0.01,\n",
    "          'force_row_wise': True,\n",
    "          'random_state': 0}\n",
    "\n",
    "oof_val_preds = np.zeros(X.shape[0])\n",
    "oof_test_preds = np.zeros(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## 폴드 1 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1095\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[681]\tvalid_0's binary_logloss: 0.151659\tvalid_0's gini: 0.289034\n",
      "폴드 1, 지니계수 0.007445387011907078\n",
      "\n",
      "######################################## 폴드 2 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1093\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[983]\tvalid_0's binary_logloss: 0.152101\tvalid_0's gini: 0.275121\n",
      "폴드 2, 지니계수 -0.0025369506444853237\n",
      "\n",
      "######################################## 폴드 3 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1097\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[560]\tvalid_0's binary_logloss: 0.151737\tvalid_0's gini: 0.280598\n",
      "폴드 3, 지니계수 0.014768166123789707\n",
      "\n",
      "######################################## 폴드 4 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1096\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[522]\tvalid_0's binary_logloss: 0.152074\tvalid_0's gini: 0.270749\n",
      "폴드 4, 지니계수 0.008141239670857461\n",
      "\n",
      "######################################## 폴드 5 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[583]\tvalid_0's binary_logloss: 0.15198\tvalid_0's gini: 0.287804\n",
      "폴드 5, 지니계수 -0.004461509808600713\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "for idx, (train_idx, valid_idx) in enumerate(folds.split(X,y)):\n",
    "    print(\"#\"*40, f'폴드 {idx + 1} / 폴드 {folds.n_splits}', \"#\"*40)\n",
    "\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "\n",
    "    dtrain = lgb.Dataset(X_train, y_train)\n",
    "    dvalid = lgb.Dataset(X_valid, y_valid)\n",
    "\n",
    "    lgb_model = lgb.train(\n",
    "        params=params,\n",
    "        train_set=dtrain,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=dvalid,\n",
    "        feval=gini,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=100)],\n",
    "    )\n",
    "\n",
    "    oof_test_preds += lgb_model.predict(X_test) / folds.n_splits\n",
    "    oof_val_preds[valid_idx] += lgb_model.predict(X_valid)\n",
    "\n",
    "    gini_score = eval_gini(y_valid, oof_test_preds[valid_idx])\n",
    "    print(f'폴드 {idx+1}, 지니계수 {gini_score}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = oof_test_preds\n",
    "submission.to_csv(data_path + 'submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 개선1. 최적화 및 피쳐 엔지니어링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 피쳐 엔지니어링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = '../data/porto-seguro-safe-driver-prediction/'\n",
    "\n",
    "train = pd.read_csv(data_path + 'train.csv', index_col = 'id')\n",
    "test = pd.read_csv(data_path + 'test.csv', index_col = 'id')\n",
    "submission = pd.read_csv(data_path + 'sample_submission.csv', index_col= 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train, test], ignore_index=True)\n",
    "all_data = all_data.drop('target', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1488028x184 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 20832392 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_features = [feature for feature in all_features if 'cat' in feature]\n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "encoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features])\n",
    "\n",
    "encoded_cat_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          1\n",
       "1          2\n",
       "2          3\n",
       "3          0\n",
       "4          2\n",
       "          ..\n",
       "1488023    1\n",
       "1488024    1\n",
       "1488025    2\n",
       "1488026    1\n",
       "1488027    0\n",
       "Name: num_missing, Length: 1488028, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 피쳐엔지니어링\n",
    "\n",
    "## IDEA 1. 데이터 하나당 결측값 개수를 파생피쳐로 사용한다면?\n",
    "all_data['num_missing'] = (all_data == -1).sum(axis=1) # 여기서 -1이 결측값임에 주의\n",
    "all_data['num_missing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_features = [feature for feature in all_features\n",
    "                      if ('cat' not in feature and 'calc' not in feature)]\n",
    "\n",
    "remaining_features.append('num_missing') # num_missing도 새로운 피쳐로 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IDEA 2. mix_ind\n",
    "\n",
    "ind_features = [feature for feature in all_features if 'ind' in feature]\n",
    "\n",
    "is_first_feature = True\n",
    "for ind_feature in ind_features:\n",
    "    if is_first_feature:\n",
    "        all_data['mix_ind'] = all_data[ind_feature].astype(str) + '_'\n",
    "        is_first_feature = False\n",
    "    else:\n",
    "        all_data['mix_ind'] += all_data[ind_feature].astype(str) + '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          2_2_5_1_0_0_1_0_0_0_0_0_0_0_11_0_1_0_\n",
       "1           1_1_7_0_0_0_0_1_0_0_0_0_0_0_3_0_0_1_\n",
       "2          5_4_9_1_0_0_0_1_0_0_0_0_0_0_12_1_0_0_\n",
       "3           0_1_2_0_0_1_0_0_0_0_0_0_0_0_8_1_0_0_\n",
       "4           0_2_0_1_0_1_0_0_0_0_0_0_0_0_9_1_0_0_\n",
       "                           ...                  \n",
       "1488023     0_1_6_0_0_0_1_0_0_0_0_0_0_0_2_0_0_1_\n",
       "1488024    5_3_5_1_0_0_0_1_0_0_0_0_0_0_11_1_0_0_\n",
       "1488025     0_1_5_0_0_1_0_0_0_0_0_0_0_0_5_0_0_1_\n",
       "1488026    6_1_5_1_0_0_0_0_1_0_0_0_0_0_13_1_0_0_\n",
       "1488027    7_1_4_1_0_0_0_0_1_0_0_0_0_0_12_1_0_0_\n",
       "Name: mix_ind, Length: 1488028, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['mix_ind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1079327, 2: 309747, 3: 70172, 4: 28259, -1: 523}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['ps_ind_02_cat'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           309747\n",
       "1          1079327\n",
       "2            28259\n",
       "3          1079327\n",
       "4           309747\n",
       "            ...   \n",
       "1488023    1079327\n",
       "1488024      70172\n",
       "1488025    1079327\n",
       "1488026    1079327\n",
       "1488027    1079327\n",
       "Name: ps_ind_02_cat_count, Length: 1488028, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_count_features = []\n",
    "\n",
    "for feature in cat_features+['mix_ind']:\n",
    "    val_counts_dict = all_data[feature].value_counts().to_dict()\n",
    "    all_data[f'{feature}_count'] = all_data[feature].apply(lambda x: val_counts_dict[x])\n",
    "\n",
    "    cat_count_features.append(f'{feature}_count')\n",
    "all_data['ps_ind_02_cat_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "drop_features = ['ps_ind_14', 'ps_ind_10_bin', 'ps_ind_11_bin',\n",
    "                 'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_car_14']\n",
    "\n",
    "all_data_remaining = all_data[remaining_features+cat_count_features].drop(drop_features, axis=1)\n",
    "\n",
    "all_data_sprs = sparse.hstack([sparse.csr_matrix(all_data_remaining),\n",
    "                               encoded_cat_matrix], format='csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 하이퍼 파라미터 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y,\n",
    "                                                      test_size=0.2,\n",
    "                                                      random_state=0)\n",
    "\n",
    "bayes_dtrain = lgb.Dataset(X_train, y_train)\n",
    "bayes_dvalid = lgb.Dataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_bounds = {'num_leaves': (30,40),\n",
    "                'lambda_l1': (0.7, 0.9),\n",
    "                'lambda_l2': (0.9, 1),\n",
    "                'feature_fraction': (0.6, 0.7),\n",
    "                'bagging_fraction': (0.6, 0.9),\n",
    "                'min_child_samples': (6, 10),\n",
    "                'min_child_weight': (10, 40)}\n",
    "\n",
    "fixed_params = {'objective': 'binary',\n",
    "                'learning_rate': 0.005,\n",
    "                'bagging_freq': 1,\n",
    "                'force_row_wise': True,\n",
    "                'random_state': 1991}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_function(num_leaves, lambda_l1, lambda_l2, feature_fraction,\n",
    "                  bagging_fraction, min_child_samples, min_child_weight):\n",
    "    '''\n",
    "    최적화하려는 지니계수를 계산하는 함수\n",
    "    '''\n",
    "    params = {'num_leaves': int(round(num_leaves)),\n",
    "                'lambda_l1': lambda_l1,\n",
    "                'lambda_l2': lambda_l2,\n",
    "                'feature_fraction': feature_fraction,\n",
    "                'bagging_fraction': bagging_fraction,\n",
    "                'min_child_samples': int(round(min_child_samples)),\n",
    "                'min_child_weight': min_child_weight,\n",
    "                'feature_pre_filter': False}\n",
    "    \n",
    "    params.update(fixed_params)\n",
    "    print('하이퍼파라미터:', params)\n",
    "\n",
    "    lgb_model = lgb.train(params=params,\n",
    "                          train_set=bayes_dtrain,\n",
    "                          num_boost_round=2500,\n",
    "                          valid_sets=bayes_dvalid,\n",
    "                          feval=gini,\n",
    "                          callbacks=[lgb.early_stopping(stopping_rounds=100)])\n",
    "    preds = lgb_model.predict(X_valid)\n",
    "\n",
    "    gini_score = eval_gini(y_valid, preds)\n",
    "\n",
    "    print(f'지니계수: {gini_score}\\n')\n",
    "\n",
    "    return gini_score\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bayesian-optimization in /Users/jhkim/anaconda3/lib/python3.11/site-packages (1.5.1)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.6 in /Users/jhkim/anaconda3/lib/python3.11/site-packages (from bayesian-optimization) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.25 in /Users/jhkim/anaconda3/lib/python3.11/site-packages (from bayesian-optimization) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn<2.0.0,>=1.0.0 in /Users/jhkim/anaconda3/lib/python3.11/site-packages (from bayesian-optimization) (1.5.1)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.0.0 in /Users/jhkim/anaconda3/lib/python3.11/site-packages (from bayesian-optimization) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/jhkim/anaconda3/lib/python3.11/site-packages (from scikit-learn<2.0.0,>=1.0.0->bayesian-optimization) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/jhkim/anaconda3/lib/python3.11/site-packages (from scikit-learn<2.0.0,>=1.0.0->bayesian-optimization) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "optimizer = BayesianOptimization(f=eval_function,\n",
    "                                 pbounds = param_bounds,\n",
    "                                 random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | lambda_l1 | lambda_l2 | min_ch... | min_ch... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "하이퍼파라미터: {'num_leaves': 34, 'lambda_l1': 0.8205526752143287, 'lambda_l2': 0.9544883182996897, 'feature_fraction': 0.6715189366372419, 'bagging_fraction': 0.7646440511781974, 'min_child_samples': 8, 'min_child_weight': 29.376823391999682, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2320]\tvalid_0's binary_logloss: 0.150941\tvalid_0's gini: 0.286136\n",
      "지니계수: 0.2861355766668298\n",
      "\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.2861   \u001b[39m | \u001b[39m0.7646   \u001b[39m | \u001b[39m0.6715   \u001b[39m | \u001b[39m0.8206   \u001b[39m | \u001b[39m0.9545   \u001b[39m | \u001b[39m7.695    \u001b[39m | \u001b[39m29.38    \u001b[39m | \u001b[39m34.38    \u001b[39m |\n",
      "하이퍼파라미터: {'num_leaves': 39, 'lambda_l1': 0.7766883037651555, 'lambda_l2': 0.9791725038082665, 'feature_fraction': 0.6963662760501029, 'bagging_fraction': 0.8675319002346239, 'min_child_samples': 8, 'min_child_weight': 27.04133683281797, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1962]\tvalid_0's binary_logloss: 0.150948\tvalid_0's gini: 0.285624\n",
      "지니계수: 0.28562419966685176\n",
      "\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m0.2856   \u001b[39m | \u001b[39m0.8675   \u001b[39m | \u001b[39m0.6964   \u001b[39m | \u001b[39m0.7767   \u001b[39m | \u001b[39m0.9792   \u001b[39m | \u001b[39m8.116    \u001b[39m | \u001b[39m27.04    \u001b[39m | \u001b[39m39.26    \u001b[39m |\n",
      "하이퍼파라미터: {'num_leaves': 40, 'lambda_l1': 0.7040436794880651, 'lambda_l2': 0.9832619845547939, 'feature_fraction': 0.608712929970154, 'bagging_fraction': 0.6213108174593661, 'min_child_samples': 9, 'min_child_weight': 36.10036444740457, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1937]\tvalid_0's binary_logloss: 0.150897\tvalid_0's gini: 0.287462\n",
      "지니계수: 0.2874621540948013\n",
      "\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m0.2875   \u001b[39m | \u001b[35m0.6213   \u001b[39m | \u001b[35m0.6087   \u001b[39m | \u001b[35m0.704    \u001b[39m | \u001b[35m0.9833   \u001b[39m | \u001b[35m9.113    \u001b[39m | \u001b[35m36.1     \u001b[39m | \u001b[35m39.79    \u001b[39m |\n",
      "하이퍼파라미터: {'num_leaves': 40, 'lambda_l1': 0.7916466541522503, 'lambda_l2': 0.9730690455162145, 'feature_fraction': 0.6621422786612307, 'bagging_fraction': 0.8892818808575982, 'min_child_samples': 8, 'min_child_weight': 39.08404679639903, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1987]\tvalid_0's binary_logloss: 0.150912\tvalid_0's gini: 0.287325\n",
      "지니계수: 0.2873247002466504\n",
      "\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.2873   \u001b[39m | \u001b[39m0.8893   \u001b[39m | \u001b[39m0.6621   \u001b[39m | \u001b[39m0.7916   \u001b[39m | \u001b[39m0.9731   \u001b[39m | \u001b[39m8.5      \u001b[39m | \u001b[39m39.08    \u001b[39m | \u001b[39m39.55    \u001b[39m |\n",
      "하이퍼파라미터: {'num_leaves': 32, 'lambda_l1': 0.9, 'lambda_l2': 0.9, 'feature_fraction': 0.6527431276993401, 'bagging_fraction': 0.6464109644733792, 'min_child_samples': 10, 'min_child_weight': 39.37146136061922, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1778]\tvalid_0's binary_logloss: 0.150962\tvalid_0's gini: 0.285421\n",
      "지니계수: 0.2854210131750523\n",
      "\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.2854   \u001b[39m | \u001b[39m0.6464   \u001b[39m | \u001b[39m0.6527   \u001b[39m | \u001b[39m0.9      \u001b[39m | \u001b[39m0.9      \u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m39.37    \u001b[39m | \u001b[39m31.79    \u001b[39m |\n",
      "하이퍼파라미터: {'num_leaves': 40, 'lambda_l1': 0.7, 'lambda_l2': 1.0, 'feature_fraction': 0.6, 'bagging_fraction': 0.6, 'min_child_samples': 6, 'min_child_weight': 36.385860573328806, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1750]\tvalid_0's binary_logloss: 0.150911\tvalid_0's gini: 0.286378\n",
      "지니계수: 0.28637752503867436\n",
      "\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.2864   \u001b[39m | \u001b[39m0.6      \u001b[39m | \u001b[39m0.6      \u001b[39m | \u001b[39m0.7      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m6.0      \u001b[39m | \u001b[39m36.39    \u001b[39m | \u001b[39m40.0     \u001b[39m |\n",
      "하이퍼파라미터: {'num_leaves': 38, 'lambda_l1': 0.9, 'lambda_l2': 0.9, 'feature_fraction': 0.7, 'bagging_fraction': 0.9, 'min_child_samples': 10, 'min_child_weight': 37.30328612286411, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1719]\tvalid_0's binary_logloss: 0.150963\tvalid_0's gini: 0.285708\n",
      "지니계수: 0.28570804760599383\n",
      "\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.2857   \u001b[39m | \u001b[39m0.9      \u001b[39m | \u001b[39m0.7      \u001b[39m | \u001b[39m0.9      \u001b[39m | \u001b[39m0.9      \u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m37.3     \u001b[39m | \u001b[39m37.85    \u001b[39m |\n",
      "하이퍼파라미터: {'num_leaves': 35, 'lambda_l1': 0.791032006952993, 'lambda_l2': 0.9072766380515447, 'feature_fraction': 0.666528224256616, 'bagging_fraction': 0.8553146515367057, 'min_child_samples': 10, 'min_child_weight': 36.468708996519126, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2069]\tvalid_0's binary_logloss: 0.150935\tvalid_0's gini: 0.285979\n",
      "지니계수: 0.28597912197637426\n",
      "\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m0.286    \u001b[39m | \u001b[39m0.8553   \u001b[39m | \u001b[39m0.6665   \u001b[39m | \u001b[39m0.791    \u001b[39m | \u001b[39m0.9073   \u001b[39m | \u001b[39m9.926    \u001b[39m | \u001b[39m36.47    \u001b[39m | \u001b[39m35.36    \u001b[39m |\n",
      "하이퍼파라미터: {'num_leaves': 40, 'lambda_l1': 0.756439416973301, 'lambda_l2': 1.0, 'feature_fraction': 0.6611305332977501, 'bagging_fraction': 0.673728415235847, 'min_child_samples': 9, 'min_child_weight': 36.15276016971561, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2052]\tvalid_0's binary_logloss: 0.15093\tvalid_0's gini: 0.286581\n",
      "지니계수: 0.28658082475107066\n",
      "\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.2866   \u001b[39m | \u001b[39m0.6737   \u001b[39m | \u001b[39m0.6611   \u001b[39m | \u001b[39m0.7564   \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m9.165    \u001b[39m | \u001b[39m36.15    \u001b[39m | \u001b[39m39.84    \u001b[39m |\n",
      "=============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "optimizer.maximize(init_points=3, n_iter=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_params = optimizer.max['params']\n",
    "max_params['num_leaves'] = int(round(max_params['num_leaves']))\n",
    "max_params['min_child_samples'] = int(round(max_params['min_child_samples']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.6213108174593661,\n",
       " 'feature_fraction': 0.608712929970154,\n",
       " 'lambda_l1': 0.7040436794880651,\n",
       " 'lambda_l2': 0.9832619845547939,\n",
       " 'min_child_samples': 9,\n",
       " 'min_child_weight': 36.10036444740457,\n",
       " 'num_leaves': 40,\n",
       " 'objective': 'binary',\n",
       " 'learning_rate': 0.005,\n",
       " 'bagging_freq': 1,\n",
       " 'force_row_wise': True,\n",
       " 'random_state': 1991}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_params.update(fixed_params)\n",
    "max_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## 폴드 1 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1099\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.15148\tvalid_0's gini: 0.29614\n",
      "폴드 1, 지니계수 0.00784703807985343\n",
      "\n",
      "######################################## 폴드 2 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1104\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.151904\tvalid_0's gini: 0.28106\n",
      "폴드 2, 지니계수 -0.0004026355954812499\n",
      "\n",
      "######################################## 폴드 3 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1101\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.151604\tvalid_0's gini: 0.284042\n",
      "폴드 3, 지니계수 0.015549803946233696\n",
      "\n",
      "######################################## 폴드 4 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1099\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 200\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\tvalid_0's binary_logloss: 0.151834\tvalid_0's gini: 0.278498\n",
      "폴드 4, 지니계수 0.009515736480726362\n",
      "\n",
      "######################################## 폴드 5 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1102\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's binary_logloss: 0.151771\tvalid_0's gini: 0.293203\n",
      "폴드 5, 지니계수 -0.005613847048018139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1991)\n",
    "\n",
    "oof_val_preds = np.zeros(X.shape[0])\n",
    "oof_test_preds = np.zeros(X_test.shape[0])\n",
    "\n",
    "for idx, (train_idx, valid_idx) in enumerate(folds.split(X,y)):\n",
    "    print(\"#\"*40, f'폴드 {idx + 1} / 폴드 {folds.n_splits}', \"#\"*40)\n",
    "\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "\n",
    "    dtrain = lgb.Dataset(X_train, y_train)\n",
    "    dvalid = lgb.Dataset(X_valid, y_valid)\n",
    "\n",
    "    lgb_model = lgb.train(\n",
    "        params=max_params,\n",
    "        train_set=dtrain,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=dvalid,\n",
    "        feval=gini,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=100)],\n",
    "    )\n",
    "\n",
    "    oof_test_preds += lgb_model.predict(X_test) / folds.n_splits\n",
    "    oof_val_preds[valid_idx] += lgb_model.predict(X_valid)\n",
    "\n",
    "    gini_score = eval_gini(y_valid, oof_test_preds[valid_idx])\n",
    "    print(f'폴드 {idx+1}, 지니계수 {gini_score}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = oof_test_preds\n",
    "submission.to_csv(data_path + 'submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 성능 개선 2. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'gini', eval_gini(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y,\n",
    "                                                      test_size=0.2,\n",
    "                                                      random_state=0)\n",
    "\n",
    "bayes_dtrain = xgb.DMatrix(X_train ,y_train)\n",
    "bayes_dvalid = xgb.DMatrix(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_bounds = {'max_depth': (4, 8),\n",
    "                'subsample': (0.6, 0.9),\n",
    "                'colsample_bytree': (0.7, 1.0),\n",
    "                'min_child_weight': (10, 40),\n",
    "                'gamma': (8, 11),\n",
    "                'reg_alpha': (7, 9),\n",
    "                'reg_lambda': (1.1, 1.5),\n",
    "                'scale_pos_weight': (1.4, 1.6)\n",
    "                }\n",
    "\n",
    "fixed_params = {'objective': 'binary:logistic',\n",
    "                'learning_rate': 0.02,\n",
    "                'random_state': 1991}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_function(max_depth, subsample, colsample_bytree, min_child_weight,\n",
    "                  reg_alpha, gamma, reg_lambda, scale_pos_weight):\n",
    "    '''\n",
    "    최적화하려는 지니계수를 계산하는 함수\n",
    "    '''\n",
    "    params = {'max_depth': int(round(max_depth)),\n",
    "              'subsample': subsample,\n",
    "              'colsample_bytree': colsample_bytree,\n",
    "              'min_child_weight': min_child_weight,\n",
    "              'gamma': gamma,\n",
    "              'reg_alpha': reg_alpha,\n",
    "              'reg_lambda': reg_lambda,\n",
    "              'scale_pos_weight': scale_pos_weight\n",
    "             }\n",
    "    \n",
    "    params.update(fixed_params)\n",
    "    print('하이퍼파라미터:', params)\n",
    "\n",
    "    xgb_model = xgb.train(params=params,\n",
    "                          dtrain=bayes_dtrain,\n",
    "                          num_boost_round=2000,\n",
    "                          evals=[(bayes_dvalid, 'bayes_dvalid')],\n",
    "                          maximize=True,\n",
    "                          feval=gini,\n",
    "                          early_stopping_rounds=100,\n",
    "                          verbose_eval=False)\n",
    "    \n",
    "    best_iter = xgb_model.best_iteration\n",
    "    preds = xgb_model.predict(bayes_dvalid, iteration_range=(0, best_iter))\n",
    "    gini_score = eval_gini(y_valid, preds)\n",
    "\n",
    "    print(f'지니계수: {gini_score}\\n')\n",
    "\n",
    "    return gini_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "optimizer = BayesianOptimization(f=eval_function,\n",
    "                                 pbounds = param_bounds,\n",
    "                                 random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |   gamma   | max_depth | min_ch... | reg_alpha | reg_la... | scale_... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "하이퍼파라미터: {'max_depth': 6, 'subsample': 0.8675319002346239, 'colsample_bytree': 0.8646440511781974, 'min_child_weight': 26.346495489906907, 'gamma': 10.14556809911726, 'reg_alpha': 7.84730959867781, 'reg_lambda': 1.3583576452266626, 'scale_pos_weight': 1.4875174422525386, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jhkim/anaconda3/lib/python3.11/site-packages/xgboost/training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수: 0.2750366949976141\n",
      "\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m0.275    \u001b[39m | \u001b[39m0.8646   \u001b[39m | \u001b[39m10.15    \u001b[39m | \u001b[39m6.411    \u001b[39m | \u001b[39m26.35    \u001b[39m | \u001b[39m7.847    \u001b[39m | \u001b[39m1.358    \u001b[39m | \u001b[39m1.488    \u001b[39m | \u001b[39m0.8675   \u001b[39m |\n",
      "하이퍼파라미터: {'max_depth': 7, 'subsample': 0.6261387899104622, 'colsample_bytree': 0.9890988281503088, 'min_child_weight': 25.866847592587135, 'gamma': 9.150324556477333, 'reg_alpha': 8.136089122187865, 'reg_lambda': 1.4702386553170643, 'scale_pos_weight': 1.4142072116395774, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n",
      "지니계수: 0.2778881193365795\n",
      "\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m0.2779   \u001b[39m | \u001b[35m0.9891   \u001b[39m | \u001b[35m9.15     \u001b[39m | \u001b[35m7.167    \u001b[39m | \u001b[35m25.87    \u001b[39m | \u001b[35m8.136    \u001b[39m | \u001b[35m1.47     \u001b[39m | \u001b[35m1.414    \u001b[39m | \u001b[35m0.6261   \u001b[39m |\n",
      "하이퍼파라미터: {'max_depth': 7, 'subsample': 0.8341587528859367, 'colsample_bytree': 0.7060655192320977, 'min_child_weight': 36.10036444740457, 'gamma': 10.497859536643814, 'reg_alpha': 8.957236684465528, 'reg_lambda': 1.4196634256866894, 'scale_pos_weight': 1.4922958724505864, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n",
      "지니계수: 0.2745660531509402\n",
      "\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m0.2746   \u001b[39m | \u001b[39m0.7061   \u001b[39m | \u001b[39m10.5     \u001b[39m | \u001b[39m7.113    \u001b[39m | \u001b[39m36.1     \u001b[39m | \u001b[39m8.957    \u001b[39m | \u001b[39m1.42     \u001b[39m | \u001b[39m1.492    \u001b[39m | \u001b[39m0.8342   \u001b[39m |\n",
      "하이퍼파라미터: {'max_depth': 6, 'subsample': 0.8883081733016013, 'colsample_bytree': 0.9808416339558672, 'min_child_weight': 27.825299918919118, 'gamma': 10.97435972205288, 'reg_alpha': 7.905695241341031, 'reg_lambda': 1.3398167944720876, 'scale_pos_weight': 1.4543932198027603, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jhkim/anaconda3/lib/python3.11/site-packages/xgboost/training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수: 0.27528800833025396\n",
      "\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m0.2753   \u001b[39m | \u001b[39m0.9808   \u001b[39m | \u001b[39m10.97    \u001b[39m | \u001b[39m6.231    \u001b[39m | \u001b[39m27.83    \u001b[39m | \u001b[39m7.906    \u001b[39m | \u001b[39m1.34     \u001b[39m | \u001b[39m1.454    \u001b[39m | \u001b[39m0.8883   \u001b[39m |\n",
      "하이퍼파라미터: {'max_depth': 6, 'subsample': 0.6777569933758918, 'colsample_bytree': 0.7031729260762098, 'min_child_weight': 23.360582308827137, 'gamma': 9.751364317718942, 'reg_alpha': 7.163192458885216, 'reg_lambda': 1.4257319147018181, 'scale_pos_weight': 1.4319324527229846, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jhkim/anaconda3/lib/python3.11/site-packages/xgboost/training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수: 0.2763529814820517\n",
      "\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m0.2764   \u001b[39m | \u001b[39m0.7032   \u001b[39m | \u001b[39m9.751    \u001b[39m | \u001b[39m5.878    \u001b[39m | \u001b[39m23.36    \u001b[39m | \u001b[39m7.163    \u001b[39m | \u001b[39m1.426    \u001b[39m | \u001b[39m1.432    \u001b[39m | \u001b[39m0.6778   \u001b[39m |\n",
      "하이퍼파라미터: {'max_depth': 7, 'subsample': 0.787087261792321, 'colsample_bytree': 0.8135648655592653, 'min_child_weight': 12.787024427483665, 'gamma': 9.11855921878361, 'reg_alpha': 8.114990497685058, 'reg_lambda': 1.4561707979191623, 'scale_pos_weight': 1.4876134907518463, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jhkim/anaconda3/lib/python3.11/site-packages/xgboost/training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수: 0.2774784791300434\n",
      "\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m0.2775   \u001b[39m | \u001b[39m0.8136   \u001b[39m | \u001b[39m9.119    \u001b[39m | \u001b[39m6.866    \u001b[39m | \u001b[39m12.79    \u001b[39m | \u001b[39m8.115    \u001b[39m | \u001b[39m1.456    \u001b[39m | \u001b[39m1.488    \u001b[39m | \u001b[39m0.7871   \u001b[39m |\n",
      "하이퍼파라미터: {'max_depth': 4, 'subsample': 0.7914002574584101, 'colsample_bytree': 0.8199566286759267, 'min_child_weight': 33.18226177889865, 'gamma': 10.954712589154026, 'reg_alpha': 8.383576538871093, 'reg_lambda': 1.2622476362983295, 'scale_pos_weight': 1.4446047674967932, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jhkim/anaconda3/lib/python3.11/site-packages/xgboost/training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수: 0.2739156655196047\n",
      "\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m0.2739   \u001b[39m | \u001b[39m0.82     \u001b[39m | \u001b[39m10.95    \u001b[39m | \u001b[39m4.068    \u001b[39m | \u001b[39m33.18    \u001b[39m | \u001b[39m8.384    \u001b[39m | \u001b[39m1.262    \u001b[39m | \u001b[39m1.445    \u001b[39m | \u001b[39m0.7914   \u001b[39m |\n",
      "하이퍼파라미터: {'max_depth': 7, 'subsample': 0.6377641552550046, 'colsample_bytree': 0.9998113981912331, 'min_child_weight': 25.878381343628586, 'gamma': 9.161858307518786, 'reg_alpha': 8.147589308643452, 'reg_lambda': 1.4866040460102463, 'scale_pos_weight': 1.425868266770488, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jhkim/anaconda3/lib/python3.11/site-packages/xgboost/training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수: 0.27884404083566094\n",
      "\n",
      "| \u001b[35m8        \u001b[39m | \u001b[35m0.2788   \u001b[39m | \u001b[35m0.9998   \u001b[39m | \u001b[35m9.162    \u001b[39m | \u001b[35m7.178    \u001b[39m | \u001b[35m25.88    \u001b[39m | \u001b[35m8.148    \u001b[39m | \u001b[35m1.487    \u001b[39m | \u001b[35m1.426    \u001b[39m | \u001b[35m0.6378   \u001b[39m |\n",
      "하이퍼파라미터: {'max_depth': 7, 'subsample': 0.6656131905598175, 'colsample_bytree': 0.9744808824787814, 'min_child_weight': 25.90601155804709, 'gamma': 9.189488687088284, 'reg_alpha': 8.175138838733645, 'reg_lambda': 1.4901615422175771, 'scale_pos_weight': 1.4538022059092626, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jhkim/anaconda3/lib/python3.11/site-packages/xgboost/training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수: 0.27857799669465416\n",
      "\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m0.2786   \u001b[39m | \u001b[39m0.9745   \u001b[39m | \u001b[39m9.189    \u001b[39m | \u001b[39m7.206    \u001b[39m | \u001b[39m25.91    \u001b[39m | \u001b[39m8.175    \u001b[39m | \u001b[39m1.49     \u001b[39m | \u001b[39m1.454    \u001b[39m | \u001b[39m0.6656   \u001b[39m |\n",
      "=========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "optimizer.maximize(init_points=3, n_iter=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.9998113981912331,\n",
       " 'gamma': 9.161858307518786,\n",
       " 'max_depth': 7,\n",
       " 'min_child_weight': 25.878381343628586,\n",
       " 'reg_alpha': 8.147589308643452,\n",
       " 'reg_lambda': 1.4866040460102463,\n",
       " 'scale_pos_weight': 1.425868266770488,\n",
       " 'subsample': 0.6377641552550046,\n",
       " 'objective': 'binary:logistic',\n",
       " 'learning_rate': 0.02,\n",
       " 'random_state': 1991}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_params = optimizer.max['params']\n",
    "max_params['max_depth'] = int(round(max_params['max_depth']))\n",
    "max_params.update(fixed_params)\n",
    "max_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## 폴드 1 / 폴드 5 ########################################\n",
      "[0]\tdvalid-logloss:0.21747\tdvalid-gini:0.20621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jhkim/anaconda3/lib/python3.11/site-packages/xgboost/training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tdvalid-logloss:0.16013\tdvalid-gini:0.27063\n",
      "[200]\tdvalid-logloss:0.15493\tdvalid-gini:0.28532\n",
      "[300]\tdvalid-logloss:0.15421\tdvalid-gini:0.29030\n",
      "[400]\tdvalid-logloss:0.15408\tdvalid-gini:0.29198\n",
      "[500]\tdvalid-logloss:0.15404\tdvalid-gini:0.29270\n",
      "[600]\tdvalid-logloss:0.15401\tdvalid-gini:0.29304\n",
      "[700]\tdvalid-logloss:0.15400\tdvalid-gini:0.29354\n",
      "[800]\tdvalid-logloss:0.15399\tdvalid-gini:0.29379\n",
      "[900]\tdvalid-logloss:0.15401\tdvalid-gini:0.29395\n",
      "[1000]\tdvalid-logloss:0.15398\tdvalid-gini:0.29405\n",
      "[1100]\tdvalid-logloss:0.15398\tdvalid-gini:0.29414\n",
      "[1200]\tdvalid-logloss:0.15398\tdvalid-gini:0.29424\n",
      "[1300]\tdvalid-logloss:0.15398\tdvalid-gini:0.29438\n",
      "[1400]\tdvalid-logloss:0.15396\tdvalid-gini:0.29447\n",
      "[1500]\tdvalid-logloss:0.15396\tdvalid-gini:0.29453\n",
      "[1600]\tdvalid-logloss:0.15396\tdvalid-gini:0.29466\n",
      "[1700]\tdvalid-logloss:0.15396\tdvalid-gini:0.29482\n",
      "[1800]\tdvalid-logloss:0.15395\tdvalid-gini:0.29490\n",
      "[1900]\tdvalid-logloss:0.15394\tdvalid-gini:0.29489\n",
      "[1999]\tdvalid-logloss:0.15395\tdvalid-gini:0.29497\n",
      "폴드 1, 지니계수 0.008828274479911538\n",
      "\n",
      "######################################## 폴드 2 / 폴드 5 ########################################\n",
      "[0]\tdvalid-logloss:0.21747\tdvalid-gini:0.19745\n",
      "[100]\tdvalid-logloss:0.16037\tdvalid-gini:0.25682\n",
      "[200]\tdvalid-logloss:0.15532\tdvalid-gini:0.26915\n",
      "[300]\tdvalid-logloss:0.15466\tdvalid-gini:0.27330\n",
      "[400]\tdvalid-logloss:0.15455\tdvalid-gini:0.27499\n",
      "[500]\tdvalid-logloss:0.15453\tdvalid-gini:0.27540\n",
      "[600]\tdvalid-logloss:0.15452\tdvalid-gini:0.27583\n",
      "[700]\tdvalid-logloss:0.15451\tdvalid-gini:0.27625\n",
      "[800]\tdvalid-logloss:0.15450\tdvalid-gini:0.27659\n",
      "[900]\tdvalid-logloss:0.15450\tdvalid-gini:0.27688\n",
      "[1000]\tdvalid-logloss:0.15448\tdvalid-gini:0.27700\n",
      "[1100]\tdvalid-logloss:0.15446\tdvalid-gini:0.27704\n",
      "[1200]\tdvalid-logloss:0.15445\tdvalid-gini:0.27717\n",
      "[1300]\tdvalid-logloss:0.15447\tdvalid-gini:0.27723\n",
      "[1400]\tdvalid-logloss:0.15447\tdvalid-gini:0.27729\n",
      "[1500]\tdvalid-logloss:0.15447\tdvalid-gini:0.27738\n",
      "[1600]\tdvalid-logloss:0.15446\tdvalid-gini:0.27742\n",
      "[1700]\tdvalid-logloss:0.15448\tdvalid-gini:0.27743\n",
      "[1800]\tdvalid-logloss:0.15446\tdvalid-gini:0.27752\n",
      "[1900]\tdvalid-logloss:0.15446\tdvalid-gini:0.27752\n",
      "[1999]\tdvalid-logloss:0.15446\tdvalid-gini:0.27762\n",
      "폴드 2, 지니계수 -0.0030604500549422\n",
      "\n",
      "######################################## 폴드 3 / 폴드 5 ########################################\n",
      "[0]\tdvalid-logloss:0.21746\tdvalid-gini:0.19021\n",
      "[100]\tdvalid-logloss:0.16009\tdvalid-gini:0.26871\n",
      "[200]\tdvalid-logloss:0.15492\tdvalid-gini:0.28051\n",
      "[300]\tdvalid-logloss:0.15424\tdvalid-gini:0.28307\n",
      "[400]\tdvalid-logloss:0.15414\tdvalid-gini:0.28369\n",
      "[500]\tdvalid-logloss:0.15411\tdvalid-gini:0.28375\n",
      "[600]\tdvalid-logloss:0.15410\tdvalid-gini:0.28399\n",
      "[700]\tdvalid-logloss:0.15409\tdvalid-gini:0.28396\n",
      "[800]\tdvalid-logloss:0.15411\tdvalid-gini:0.28404\n",
      "[900]\tdvalid-logloss:0.15408\tdvalid-gini:0.28419\n",
      "[1000]\tdvalid-logloss:0.15409\tdvalid-gini:0.28423\n",
      "[1100]\tdvalid-logloss:0.15408\tdvalid-gini:0.28430\n",
      "[1200]\tdvalid-logloss:0.15408\tdvalid-gini:0.28434\n",
      "[1300]\tdvalid-logloss:0.15409\tdvalid-gini:0.28429\n",
      "[1400]\tdvalid-logloss:0.15408\tdvalid-gini:0.28435\n",
      "[1421]\tdvalid-logloss:0.15408\tdvalid-gini:0.28435\n",
      "폴드 3, 지니계수 0.014675410289718253\n",
      "\n",
      "######################################## 폴드 4 / 폴드 5 ########################################\n",
      "[0]\tdvalid-logloss:0.21746\tdvalid-gini:0.20358\n",
      "[100]\tdvalid-logloss:0.16017\tdvalid-gini:0.25964\n",
      "[200]\tdvalid-logloss:0.15511\tdvalid-gini:0.27070\n",
      "[300]\tdvalid-logloss:0.15448\tdvalid-gini:0.27312\n",
      "[400]\tdvalid-logloss:0.15438\tdvalid-gini:0.27370\n",
      "[500]\tdvalid-logloss:0.15437\tdvalid-gini:0.27402\n",
      "[600]\tdvalid-logloss:0.15434\tdvalid-gini:0.27427\n",
      "[700]\tdvalid-logloss:0.15434\tdvalid-gini:0.27440\n",
      "[800]\tdvalid-logloss:0.15435\tdvalid-gini:0.27448\n",
      "[900]\tdvalid-logloss:0.15435\tdvalid-gini:0.27463\n",
      "[1000]\tdvalid-logloss:0.15432\tdvalid-gini:0.27465\n",
      "[1100]\tdvalid-logloss:0.15434\tdvalid-gini:0.27466\n",
      "[1200]\tdvalid-logloss:0.15434\tdvalid-gini:0.27464\n",
      "[1207]\tdvalid-logloss:0.15434\tdvalid-gini:0.27464\n",
      "폴드 4, 지니계수 0.008336623877407658\n",
      "\n",
      "######################################## 폴드 5 / 폴드 5 ########################################\n",
      "[0]\tdvalid-logloss:0.21748\tdvalid-gini:0.18835\n",
      "[100]\tdvalid-logloss:0.16029\tdvalid-gini:0.26848\n",
      "[200]\tdvalid-logloss:0.15514\tdvalid-gini:0.28274\n",
      "[300]\tdvalid-logloss:0.15444\tdvalid-gini:0.28700\n",
      "[400]\tdvalid-logloss:0.15434\tdvalid-gini:0.28846\n",
      "[500]\tdvalid-logloss:0.15430\tdvalid-gini:0.28878\n",
      "[600]\tdvalid-logloss:0.15428\tdvalid-gini:0.28901\n",
      "[700]\tdvalid-logloss:0.15427\tdvalid-gini:0.28927\n",
      "[800]\tdvalid-logloss:0.15427\tdvalid-gini:0.28940\n",
      "[900]\tdvalid-logloss:0.15426\tdvalid-gini:0.28945\n",
      "[1000]\tdvalid-logloss:0.15427\tdvalid-gini:0.28971\n",
      "[1100]\tdvalid-logloss:0.15426\tdvalid-gini:0.28979\n",
      "[1200]\tdvalid-logloss:0.15426\tdvalid-gini:0.28991\n",
      "[1300]\tdvalid-logloss:0.15423\tdvalid-gini:0.29012\n",
      "[1400]\tdvalid-logloss:0.15425\tdvalid-gini:0.29023\n",
      "[1500]\tdvalid-logloss:0.15423\tdvalid-gini:0.29038\n",
      "[1600]\tdvalid-logloss:0.15424\tdvalid-gini:0.29036\n",
      "[1696]\tdvalid-logloss:0.15422\tdvalid-gini:0.29037\n",
      "폴드 5, 지니계수 -0.0053467805750307545\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1991)\n",
    "\n",
    "oof_val_preds = np.zeros(X.shape[0])\n",
    "oof_test_preds = np.zeros(X_test.shape[0])\n",
    "\n",
    "for idx, (train_idx, valid_idx) in enumerate(folds.split(X,y)):\n",
    "    print(\"#\"*40, f'폴드 {idx + 1} / 폴드 {folds.n_splits}', \"#\"*40)\n",
    "\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_valid, y_valid = X[valid_idx], y[valid_idx]\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, y_valid)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "    xgb_model = xgb.train(\n",
    "        params=max_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=2000,\n",
    "        evals=[(dvalid, 'dvalid')],\n",
    "        maximize=True,\n",
    "        feval=gini,\n",
    "        early_stopping_rounds=200,\n",
    "        verbose_eval=100\n",
    "    )\n",
    "\n",
    "    best_iter = xgb_model.best_iteration\n",
    "\n",
    "    oof_test_preds += xgb_model.predict(dtest, iteration_range=(0, best_iter)) / folds.n_splits\n",
    "    oof_val_preds[valid_idx] += xgb_model.predict(dvalid, iteration_range=(0, best_iter))\n",
    "\n",
    "    gini_score = eval_gini(y_valid, oof_test_preds[valid_idx])\n",
    "    print(f'폴드 {idx+1}, 지니계수 {gini_score}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = oof_test_preds\n",
    "submission.to_csv(data_path + 'submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 성능 개선 3. 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_test_preds = oof_test_preds_lgb * 0.5 + oof_test_preds_xgb * 0.5\n",
    "\n",
    "submission['target'] = oof_test_preds\n",
    "submission.to_csv(data_path + 'submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
